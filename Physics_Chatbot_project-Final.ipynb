{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465759c9",
   "metadata": {},
   "source": [
    "# SCI-BOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdcd0bf",
   "metadata": {},
   "source": [
    "This code involves the development of a chatbot for discussing scientific topics, integrating various libraries and services for seamless functionality. Initially, configurations are set up, including API keys for OpenAI and Pinecone services. The chatbot leverages the power of OpenAI's GPT-3.5 model for natural language processing, enabling smooth conversational flow. It loads and preprocesses a dataset named \"ronaldahmed/scitechnews,\". Integration with Pinecone allows for efficient vector storage and similarity searches, essential for handling user queries effectively. The project creates and initializes a Pinecone index named \"science-papers-1,\" ensuring it's ready for use. An embedding model from OpenAI converts text data into numerical representations for processing. Furthermore, a prompt augmentation function enriches user queries with relevant context from the Pinecone knowledge base. With Gradio, the project provides a user-friendly interface for users to interact with the chatbot, fostering engagement and knowledge dissemination in scientific domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b6ad5c",
   "metadata": {},
   "source": [
    "### Installing Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    langchain==0.0.354 \\\n",
    "    openai==1.10.0 \\\n",
    "    datasets==2.10.1 \\\n",
    "    pinecone-client==3.0.0 \\\n",
    "    tiktoken==0.5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa6247",
   "metadata": {},
   "source": [
    "### OPEN AI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546c02e",
   "metadata": {},
   "source": [
    "The OpenAI API key is set to authenticate access to OpenAI's services. Subsequently, GPT-3.5 turbo model is used for language processing. Additionally, message objects representing system messages, human messages, and AI messages are imported from the LangChain schema module. These messages are organized within an array, likely serving as initial conversation prompts and responses for the chatbot. Overall, this segment sets the groundwork for the chatbot's functionality, enabling it to process and generate responses based on user interactions through the OpenAI API and LangChain library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d1267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ENTER_YOUR_API_KEY\"\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7321312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand about New York City's Vaccine Passport Plan Renews Online Privacy Debate.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d22c6a7",
   "metadata": {},
   "source": [
    "### Loading Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e5ac9",
   "metadata": {},
   "source": [
    "Hugging Face Datasets library is used to load `\"ronaldahmed/scitechnews\"` dataset. \n",
    "\n",
    "Dataset overview: \n",
    "The SciTechNews dataset consists of scientific papers paired with their corresponding press release snippet mined from ACM TechNews. ACM TechNews is a news aggregator that provides regular news digests about scientific achieve- ments and technology in the areas of Computer Science, Engineering, Astrophysics, Biology, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20439fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5110ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ronaldahmed/scitechnews\")\n",
    "\n",
    "dataset_sci_data_1 = dataset['train']\n",
    "\n",
    "# Print information about the dataset\n",
    "print(dataset_sci_data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71613811",
   "metadata": {},
   "source": [
    "### Creating a Vector Database index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90812ec9",
   "metadata": {},
   "source": [
    "The below code segment performs several tasks related to organizing and processing data, particularly for indexing and embedding text information. Initially, it sets up a system to manage and measure the performance of a data index named \"science-papers-1\". It creates one with specific characteristics like dimensionality and metric, ensuring efficient data storage and retrieval. Additionally, it waits for the index to be fully initialized before proceeding. Then, it establishes a connection to the index to begin data operations. Next, it employs an embedding model, likely for converting text data into numerical representations for analysis and comparison. Overall, this code orchestrates the organization and analysis of textual data, facilitating efficient indexing and embedding for further processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "api_key = \"ENTER_YOUR_API_KEY\"\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ccbdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"gcp-starter\", region=\"Iowa (us-central1)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74191444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = 'science-papers-1'\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # dimensionality of ada 002\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c4343",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e74005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8fa028",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "\n",
    "# Assuming 'train' is the split you want to work with\n",
    "train_data = dataset_sci_data_1.to_pandas()  # Convert 'train' split to pandas DataFrame\n",
    "\n",
    "batch_size = 80\n",
    "\n",
    "for i in tqdm(range(0, len(train_data), batch_size)):\n",
    "    i_end = min(len(train_data), i + batch_size)\n",
    "    # get batch of data\n",
    "    batch = train_data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['id']}\" for _, x in batch.iterrows()]\n",
    "    # get text to embed\n",
    "    texts = [x['pr-summary'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['pr-summary'],\n",
    "         'id': x['id'],\n",
    "         'title': x['pr-title']} for _, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201fa147",
   "metadata": {},
   "source": [
    "### Similarity search and RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c15d3",
   "metadata": {},
   "source": [
    "The below code is part of a system designed to enhance the capabilities of a chatbot by integrating a vector store mechanism with the Pinecone library. Initially, a metadata field named \"text\" is defined, indicating where the summary of science papers' data resides within the system. Subsequently, a vector store object is initialized using the Pinecone library, incorporating an index, an embedding model, and the designated text field. The vector store facilitates efficient storage and retrieval of numerical representations of text data. The code also defines a function named augment_prompt, which enhances user queries by retrieving relevant contextual information from the vector store. It executes a similarity search based on the user's query, retrieves relevant results, and constructs an augmented prompt by combining these results with the original query. Additionally, there's a function named input_message that takes user messages, augments them with contextual information using the augment_prompt function, and sends the augmented message to the chatbot for processing. This mechanism enriches the chatbot's responses by incorporating relevant context retrieved from the vector store, enhancing the overall user experience and the chatbot's ability to provide informative responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d0b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6da538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 1 result from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=1)\n",
    "\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "\n",
    "    print(\"augment_prompt: \", augmented_prompt)\n",
    "    return augmented_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d504fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_message (message):\n",
    "    prompt = HumanMessage(\n",
    "        content=augment_prompt(\n",
    "            \"message\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    messages.append(prompt)\n",
    "    res = chat(messages + [prompt])\n",
    "    \n",
    "    return res.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a86b57",
   "metadata": {},
   "source": [
    "### Creating an interface using GRADIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234a745",
   "metadata": {},
   "source": [
    "The below code sets up a user interface for a chatbot using the Gradio library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ee0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pyqt5 pyqtwebengine pydantic\n",
    "! pip install --upgrade jinja2\n",
    "! pip install --upgrade gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ce093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def echo(message, history):\n",
    "    return input_message(message)\n",
    "\n",
    "demo = gr.ChatInterface(fn=echo, examples=[\"Who developed a quantum information transfer protocol ?\", \"Which organization launched the Joint Cyber Defense Collaborative (JCDC) ?\", \"Who is doing research about disease progression and response to treatment for brain disorders ?\"], title=\"Sci-Bot\")\n",
    "demo.launch(share = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
